"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[350],{3905:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>u});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),m=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},p=function(e){var t=m(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),c=m(r),f=a,u=c["".concat(l,".").concat(f)]||c[f]||d[f]||o;return r?n.createElement(u,s(s({ref:t},p),{},{components:r})):n.createElement(u,s({ref:t},p))}));function u(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,s=new Array(o);s[0]=f;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:a,s[1]=i;for(var m=2;m<o;m++)s[m]=r[m];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}f.displayName="MDXCreateElement"},9778:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>m});var n=r(7462),a=(r(7294),r(3905));const o={},s=void 0,i={unversionedId:"Transforms",id:"Transforms",title:"Transforms",description:"Learn the Basics ||",source:"@site/docs/05-Transforms.md",sourceDirName:".",slug:"/Transforms",permalink:"/pytorch-basics/docs/Transforms",draft:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Data",permalink:"/pytorch-basics/docs/Data"},next:{title:"BuildModel",permalink:"/pytorch-basics/docs/BuildModel"}},l={},m=[{value:"ToTensor()",id:"totensor",level:2},{value:"Lambda Transforms",id:"lambda-transforms",level:2},{value:"Further Reading",id:"further-reading",level:3}],p={toc:m},c="wrapper";function d(e){let{components:t,...r}=e;return(0,a.kt)(c,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"intro.html"},"Learn the Basics")," ||\n",(0,a.kt)("a",{parentName:"p",href:"quickstart_tutorial.html"},"Quickstart")," ||\n",(0,a.kt)("a",{parentName:"p",href:"tensorqs_tutorial.html"},"Tensors")," ||\n",(0,a.kt)("a",{parentName:"p",href:"data_tutorial.html"},"Datasets & DataLoaders")," ||\n",(0,a.kt)("strong",{parentName:"p"},"Transforms")," ||\n",(0,a.kt)("a",{parentName:"p",href:"buildmodel_tutorial.html"},"Build Model")," ||\n",(0,a.kt)("a",{parentName:"p",href:"autogradqs_tutorial.html"},"Autograd")," ||\n",(0,a.kt)("a",{parentName:"p",href:"optimization_tutorial.html"},"Optimization")," ||\n",(0,a.kt)("a",{parentName:"p",href:"saveloadrun_tutorial.html"},"Save & Load Model")),(0,a.kt)("h1",{id:"transforms"},"Transforms"),(0,a.kt)("p",null,"Data does not always come in its final processed form that is required for\ntraining machine learning algorithms. We use ",(0,a.kt)("strong",{parentName:"p"},"transforms")," to perform some\nmanipulation of the data and make it suitable for training."),(0,a.kt)("p",null,"All TorchVision datasets have two parameters -",(0,a.kt)("inlineCode",{parentName:"p"},"transform")," to modify the features and\n",(0,a.kt)("inlineCode",{parentName:"p"},"target_transform")," to modify the labels - that accept callables containing the transformation logic.\nThe ",(0,a.kt)("a",{parentName:"p",href:"https://pytorch.org/vision/stable/transforms.html"},"torchvision.transforms")," module offers\nseveral commonly-used transforms out of the box."),(0,a.kt)("p",null,"The FashionMNIST features are in PIL Image format, and the labels are integers.\nFor training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\nTo make these transformations, we use ",(0,a.kt)("inlineCode",{parentName:"p"},"ToTensor")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"Lambda"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'%matplotlib inline\n\nimport torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\nds = datasets.FashionMNIST(\n    root="data",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n')),(0,a.kt)("h2",{id:"totensor"},"ToTensor()"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor"},"ToTensor"),"\nconverts a PIL image or NumPy ",(0,a.kt)("inlineCode",{parentName:"p"},"ndarray")," into a ",(0,a.kt)("inlineCode",{parentName:"p"},"FloatTensor"),". and scales\nthe image's pixel intensity values in the range ","[0., 1.]"),(0,a.kt)("h2",{id:"lambda-transforms"},"Lambda Transforms"),(0,a.kt)("p",null,"Lambda transforms apply any user-defined lambda function. Here, we define a function\nto turn the integer into a one-hot encoded tensor.\nIt first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\n",(0,a.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html"},"scatter_")," which assigns a\n",(0,a.kt)("inlineCode",{parentName:"p"},"value=1")," on the index as given by the label ",(0,a.kt)("inlineCode",{parentName:"p"},"y"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"target_transform = Lambda(lambda y: torch.zeros(\n    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n")),(0,a.kt)("hr",null),(0,a.kt)("h3",{id:"further-reading"},"Further Reading"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://pytorch.org/vision/stable/transforms.html"},"torchvision.transforms API"))))}d.isMDXComponent=!0}}]);